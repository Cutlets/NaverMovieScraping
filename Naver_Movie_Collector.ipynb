{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Naver_Movie_Collector.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1lHpFyXX_bmT7qKrk_TvInzQFt6AuR7cR",
      "authorship_tag": "ABX9TyMnxHhanQBmWu7FNdzrBAd8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cutlets/NaverMovieScraping/blob/main/Naver_Movie_Collector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9y7ZlDVU3sf0"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OgQeD8QHa0p"
      },
      "source": [
        "import requests\n",
        "import urllib.request\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "import os.path\n",
        "import pprint as pp\n",
        "import unicodedata\n",
        "import shutil\n",
        "import csv\n",
        "import torch\n",
        "\n",
        "from csv import reader\n",
        "from collections import OrderedDict\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm import tqdm\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "from tokenizers import SentencePieceBPETokenizer\n",
        "from transformers import BartForConditionalGeneration"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAJuHpKR31_2",
        "outputId": "d52e39f2-4dbd-41f8-bd46-5aeb951cdf60"
      },
      "source": [
        "# Text-Summarization With KoBERT\n",
        "# Code from here : https://huggingface.co/spaces/gogamza/kobart-summarization\n",
        "def tokenizer():\n",
        "    tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-summarization')\n",
        "    return tokenizer\n",
        "\n",
        "def get_model():\n",
        "    model = BartForConditionalGeneration.from_pretrained('gogamza/kobart-summarization')\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "model = get_model()\n",
        "tokenizer = tokenizer()\n",
        "\n",
        "def tsum(txt):\n",
        "  summ = \"NaN\"\n",
        "  if txt:\n",
        "    raw_input_ids = tokenizer.encode(txt)\n",
        "    input_ids = [tokenizer.bos_token_id] + \\\n",
        "        raw_input_ids + [tokenizer.eos_token_id]\n",
        "    summary_ids = model.generate(torch.tensor([input_ids]),\n",
        "                                 max_length=256,\n",
        "                                 early_stopping=True,\n",
        "                                 repetition_penalty=2.0)\n",
        "    summ = tokenizer.decode(summary_ids.squeeze().tolist(), skip_special_tokens=True)\n",
        "  else:\n",
        "    summ = \"NaN\"\n",
        "  return summ"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
            "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9kNLWJyH0nH"
      },
      "source": [
        "# Variable Setting #\n",
        "\n",
        "# Option\n",
        "debugMode = False\n",
        "yearList = [2021, 2020, 2019, 2018, 2017, 2016]\n",
        "loadMovieData = True\n",
        "loadMovieData_DebugMode = False\n",
        "importZip = True\n",
        "\n",
        "# Debug Mode Variables\n",
        "reduceYearPage = 0\n",
        "reduceReviewPage = 0\n",
        "yRange = len(yearList)\n",
        "if debugMode:\n",
        "  reduceYearPage = 26\n",
        "  reduceReviewPage = 9\n",
        "  yRange = 1\n",
        "  loadMovieData = False or loadMovieData_DebugMode\n",
        "  importZip = False\n",
        "    \n",
        "# If has a start index. 0 = None\n",
        "minIndex = 142620\n",
        "\n",
        "# NSMC Raw Path\n",
        "os.makedirs('./movie_data/', exist_ok=True)\n",
        "rawPath = \"./movie_data/\"\n",
        "rawList = os.listdir(rawPath)\n",
        "\n",
        "# Naver movie URL\n",
        "nmURL = \"https://movie.naver.com/movie/bi/mi/basic.naver?code=\"\n",
        "nmReviewURL = \"https://movie.naver.com/movie/bi/mi/pointWriteFormList.naver?code=\"\n",
        "nmReviewURL2 = \"&type=after&isActualPointWriteExecute=false&isMileageSubscriptionAlready=false&isMileageSubscriptionReject=false\"\n",
        "nmYearURL = \"https://movie.naver.com/movie/sdb/browsing/bmovie.naver?open=\"\n",
        "\n",
        "nmPage = \"&page=\"\n",
        "maxYearPage = 31 - reduceYearPage # N-1 is max / Max Value = 31\n",
        "maxReviewPage = 11 - reduceReviewPage # N-1 is max\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJxxWJIYJl5M"
      },
      "source": [
        "# Functions\n",
        "\n",
        "###########################\n",
        "########### UNUSED FUNCTION\n",
        "# Extract the movie ID from raw json lists\n",
        "def rawToMovieId(rawLst):\n",
        "  mLst = []\n",
        "  mid = 0\n",
        "\n",
        "  for raw in rawLst:\n",
        "    mid = re.sub('\\.json', '', raw)\n",
        "    mLst.append(int(mid))\n",
        "\n",
        "  mLst.sort()\n",
        "\n",
        "  return mLst\n",
        "\n",
        "# Read raw jsons\n",
        "def readJson(movieLst):\n",
        "  json_collection = []\n",
        "  for mid in tqdm(movieLst):\n",
        "    with open(rawPath+str(mid)+'.json', 'r') as f:\n",
        "      json_data = json.load(f)\n",
        "    json_collection.append(json_data)\n",
        "  print()\n",
        "  return json_collection\n",
        "########### UNUSED FUNCTION\n",
        "###########################\n",
        "\n",
        "# Scraping\n",
        "def getMoviedata(mid):\n",
        "  filepath = './movie_data/'+str(mid)+'.json'\n",
        "  if os.path.isfile(filepath) and loadMovieData:\n",
        "    #print(\"Already exist json : \", filepath)\n",
        "    with open(filepath, 'r') as f:\n",
        "      json_data = json.load(f)\n",
        "      return json_data\n",
        "\n",
        "  URL = nmURL + str(mid)\n",
        "  movieDetail = requests.get(URL)\n",
        "  movieReview = requests.get\n",
        "  soup = BeautifulSoup(movieDetail.text, 'html.parser')\n",
        "  \n",
        "  # Rating\n",
        "  selector = '#pointNetizenPersentBasic'\n",
        "  if(len(soup.select(selector)) > 0):\n",
        "    rank = soup.select(selector)[0].text\n",
        "    rank = re.sub('^관람객 평점','', rank)\n",
        "    rank = re.sub('점','', rank)\n",
        "    rank = rank.strip()\n",
        "  else:\n",
        "    rank = \"NaN\"\n",
        "\n",
        "  # Genre\n",
        "  selector = '#content > div.article > div.mv_info_area > div.mv_info > dl > dd:nth-of-type(1) > p > span:nth-of-type(1)'\n",
        "  if(len(soup.select(selector)) > 0):\n",
        "    genre = soup.select(selector)[0].text\n",
        "    genre = re.sub('\\r|\\t|\\n', '', genre)\n",
        "    genre = genre.strip()\n",
        "  else:\n",
        "    genre = \"NaN\"\n",
        "\n",
        "  # PG\n",
        "  selector = '#content > div.article > div.mv_info_area > div.mv_info > dl > dd:nth-of-type(4) > p > a:nth-of-type(1)'\n",
        "  if(len(soup.select(selector)) > 0):\n",
        "    pg = soup.select(selector)[0].text\n",
        "    pg = pg.strip()\n",
        "  else:\n",
        "    pg = \"NaN\"\n",
        "\n",
        "  # Summary\n",
        "  selector = '#content > div.article > div.section_group.section_group_frst > div:nth-of-type(1) > div > div > p'\n",
        "  if(len(soup.select(selector)) > 0):\n",
        "    syn = soup.select(selector)[0].text\n",
        "    syn = unicodedata.normalize(\"NFKD\", syn)\n",
        "    syn = re.sub('\\r', '', syn)\n",
        "    syn = syn.strip()\n",
        "  else:\n",
        "    syn = \"NaN\"\n",
        "\n",
        "  # Title\n",
        "  selector = '#content > div.article > div.wide_info_area > div.mv_info > h3 > a'\n",
        "  if(len(soup.select(selector)) > 0):\n",
        "    title = soup.select(selector)[0].text\n",
        "    title = title.strip()\n",
        "  else:\n",
        "    title = \"NaN\"\n",
        "  \n",
        "  # Review own check\n",
        "  selector = '#pointNetizenCountBasic > em'\n",
        "  if(len(soup.select(selector)) > 0):\n",
        "    hasReview = True\n",
        "  else:\n",
        "    hasReview = False\n",
        "\n",
        "  # Related movie\n",
        "  selector = '#content > div.article > div:nth-of-type(5) > div:nth-of-type(2) > div > ul > li > a.title_mv'\n",
        "  rMovieList = []\n",
        "  if(len(soup.select(selector)) > 0):\n",
        "    for mv in soup.select(selector):\n",
        "      rMovieList.append(mv.text)\n",
        "  else:\n",
        "    rMovieList = np.nan\n",
        "  \n",
        "  movie_data = OrderedDict()\n",
        "  movie_data[\"Title\"] = title\n",
        "  movie_data[\"MovieID\"] = mid\n",
        "  movie_data[\"Genre\"] = genre\n",
        "  movie_data[\"PG\"] = pg\n",
        "  movie_data[\"Summary\"] = syn\n",
        "  movie_data[\"Summary_short\"] = tsum(syn)\n",
        "  movie_data[\"Rating\"] = float(rank)\n",
        "  movie_data[\"RelateMovie\"] = rMovieList\n",
        "\n",
        "  if hasReview and (pg != '청소년 관람불가') and (pg != '제한상영가') and (int(float(rank)) != 0):\n",
        "    movie_data[\"vaildReview\"] = hasReview\n",
        "  else:\n",
        "     movie_data[\"vaildReview\"] = False\n",
        "\n",
        "  os.makedirs('./movie_data/', exist_ok=True)\n",
        "  #print(\"Creating the json file...\")\n",
        "  with open(filepath, 'w', encoding=\"utf-8\") as make_file:\n",
        "    json.dump(movie_data, make_file, ensure_ascii=False, indent=\"\\t\")\n",
        "\n",
        "  with open(filepath, 'r', encoding=\"utf-8\") as f:\n",
        "    json_data = json.load(f)\n",
        "\n",
        "  return json_data\n",
        "\n",
        "\n",
        "\n",
        "# Merge movie data\n",
        "def readMoviedata(movieLst):\n",
        "  movieData = []\n",
        "  for mid in tqdm(movieLst):\n",
        "    d = getMoviedata(mid)\n",
        "    movieData.append(d)\n",
        "  print()\n",
        "  return movieData\n",
        "\n",
        "\n",
        "\n",
        "# Collect Movie Data by year\n",
        "def collectMovieByYear(yearLst):\n",
        "  movieID = []\n",
        "\n",
        "  filepath = './movie_list/movieIdList.csv'\n",
        "  if os.path.isfile(filepath) and loadMovieData:\n",
        "    with open(filepath, 'r', encoding='utf-8') as csv_file:\n",
        "      csv_reader = reader(csv_file)\n",
        "      for row in csv_reader:\n",
        "        movieID += (row)\n",
        "        return movieID\n",
        "\n",
        "  for year in tqdm(yearLst):\n",
        "    baseURL = nmYearURL + str(year)\n",
        "    for page in range(1, maxYearPage):\n",
        "      URL = baseURL + nmPage + str(page)\n",
        "      yearList = requests.get(URL)\n",
        "      soup = BeautifulSoup(yearList.text, 'html.parser')\n",
        "\n",
        "      selector = '#old_content > ul > li'\n",
        "      linkList = soup.select(selector)\n",
        "\n",
        "      for link in linkList:\n",
        "        movieID.append(link.find('a')['href'])\n",
        "  print()\n",
        "\n",
        "  for link in movieID:\n",
        "    ID = link.replace(r'/movie/bi/mi/basic.naver?code=', '')\n",
        "    movieID[movieID.index(link)] = int(ID)\n",
        "\n",
        "  movieID = set(movieID)\n",
        "  movieID = list(movieID)\n",
        "\n",
        "  for ID in movieID[:]:\n",
        "    if ID < (minIndex+1):\n",
        "      movieID.remove(ID)\n",
        "\n",
        "  movieID.sort()\n",
        "\n",
        "  listPath = './movie_list/'\n",
        "  os.makedirs(listPath, exist_ok=True)\n",
        "  with open(listPath+\"movieIdList.csv\", 'w') as file: \n",
        "    writer = csv.writer(file) \n",
        "    writer.writerow(movieID)\n",
        "  return movieID"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EmL6ZMvrCFd"
      },
      "source": [
        "# Collect Movie Reviews from mid\n",
        "def getReviewByID(mid):\n",
        "  os.makedirs('./review_data/', exist_ok=True)\n",
        "  filepath = './review_data/'+str(mid)+'.json'\n",
        "  if os.path.isfile(filepath) and loadMovieData:\n",
        "    #print(\"Already exist json : \", filepath)\n",
        "    with open(filepath, 'r') as f:\n",
        "      json_data = json.load(f)\n",
        "      return json_data\n",
        "\n",
        "  # Metadata\n",
        "  reviewURL = nmReviewURL + str(mid) + nmReviewURL2 + nmPage\n",
        "  firstURL = reviewURL + str(1)\n",
        "  reviewNum = -1\n",
        "  rPage = 1\n",
        "\n",
        "  # Infomations\n",
        "  reviewText = []\n",
        "  date = []\n",
        "  rating = []\n",
        "  author = []\n",
        "  review_id = []\n",
        "  movie_id = mid\n",
        "\n",
        "  while rPage != 0:\n",
        "    mentMax = 0\n",
        "    URL = reviewURL + str(rPage)\n",
        "    rPage += 1\n",
        "    reviewPage = requests.get(URL)\n",
        "    soup = BeautifulSoup(reviewPage.text, 'html.parser')\n",
        "\n",
        "    if reviewNum == -1:\n",
        "      selector = 'body > div > div > div.score_total > strong > em'\n",
        "      if(len(soup.select(selector)) > 0):\n",
        "        reviewNum = soup.select(selector)[0].text\n",
        "        reviewNum = reviewNum.replace(\",\",\"\")\n",
        "        reviewNum = int(reviewNum)\n",
        "      else:\n",
        "        reviewNum = 0\n",
        "\n",
        "    if reviewNum < 10 or rPage == 21:\n",
        "      rPage = 0\n",
        "      mentMax = reviewNum % 10\n",
        "    else:\n",
        "      reviewNum = reviewNum - 10\n",
        "      mentMax = 10\n",
        "\n",
        "    # Retrive Data\n",
        "    for mentNum in range(0, mentMax):\n",
        "      # Review Text\n",
        "      selector = f'#_filtered_ment_{mentNum}'\n",
        "      if(len(soup.select(selector)) > 0):\n",
        "        text = soup.select(selector)[0].text\n",
        "        text = text.strip()\n",
        "      else:\n",
        "        text = \"NaN\"\n",
        "\n",
        "      # Rating\n",
        "      selector = f'body > div > div > div.score_result > ul > li:nth-of-type({mentNum+1}) > div.star_score > em'\n",
        "      if(len(soup.select(selector)) > 0):\n",
        "        score = soup.select(selector)[0].text\n",
        "      else:\n",
        "        score = \"NaN\"\n",
        "\n",
        "      # Author\n",
        "      selector = f'body > div > div > div.score_result > ul > li:nth-of-type({mentNum+1}) > div.score_reple > dl > dt > em:nth-of-type(1) > a > span'\n",
        "      if(len(soup.select(selector)) > 0):\n",
        "        nid = soup.select(selector)[0].text\n",
        "        nid = re.sub('(^\\S*\\()|(\\)$)',\"\", nid)\n",
        "      else:\n",
        "        nid = \"NaN\"\n",
        "\n",
        "      # Date\n",
        "      selector = f'body > div > div > div.score_result > ul > li:nth-of-type({mentNum+1}) > div.score_reple > dl > dt > em:nth-of-type(2)'\n",
        "      if(len(soup.select(selector)) > 0):\n",
        "        rDate = soup.select(selector)[0].text\n",
        "        rDate = re.sub('(^\\d\\d)|(\\s\\d\\d:\\d\\d$)',\"\", rDate)\n",
        "      else:\n",
        "        rDate = \"NaN\"\n",
        "      \n",
        "      # review_id\n",
        "      selector = f'body > div > div > div.score_result > ul > li:nth-of-type({mentNum+1}) > div.score_reple > dl > dt > em:nth-of-type(1) > a'\n",
        "      if(len(soup.select(selector)) > 0):\n",
        "        rid = soup.select(selector)[0]['onclick']\n",
        "        rid = re.sub('\\D',\"\", rid)\n",
        "      else:\n",
        "        rid = \"NaN\"\n",
        "\n",
        "      reviewText.append(text)\n",
        "      rating.append(score)\n",
        "      author.append(nid)\n",
        "      date.append(rDate)\n",
        "      review_id.append(rid)\n",
        "\n",
        "  reviews = []\n",
        "\n",
        "  for Num in range(0, len(reviewText)):\n",
        "    r = OrderedDict()\n",
        "    r[\"review\"] = reviewText[Num]\n",
        "    r[\"date\"] = date[Num]\n",
        "    r[\"rating\"] = rating[Num]\n",
        "    r[\"author\"] = author[Num]\n",
        "    r[\"review_id\"] = review_id[Num]\n",
        "    r[\"movie_id\"] = str(movie_id)\n",
        "    reviews.append(r)\n",
        "\n",
        "  #print(\"Creating the json file...\")\n",
        "  with open(filepath, 'w', encoding=\"utf-8\") as make_file:\n",
        "    json.dump(reviews, make_file, ensure_ascii=False, indent=\"\\t\")\n",
        "\n",
        "  return reviews\n",
        "\n",
        "def getReviews(movieData):\n",
        "  reviews = []\n",
        "  for movie in tqdm(movieData):\n",
        "    review = getReviewByID(movie['MovieID'])\n",
        "    reviews.append(review)\n",
        "\n",
        "  return reviews"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FowhSIgbKn9Z"
      },
      "source": [
        "if __name__==\"__main__\":\n",
        "\n",
        "  if debugMode:\n",
        "    movieList = collectMovieByYear(yearList[:yRange])\n",
        "  else:\n",
        "    movieList = collectMovieByYear(yearList)\n",
        "\n",
        "  # For Debug\n",
        "  reduceList = len(movieList)\n",
        "  if debugMode:\n",
        "    reduceList = 5\n",
        "    movieList = movieList[:reduceList]\n",
        "  \n",
        "  movieData = readMoviedata(movieList)\n",
        "  reviewData = getReviews(movieData)\n",
        "\n",
        "  # Test Code\n",
        "  if debugMode:\n",
        "    pp.pprint(movieData[0])\n",
        "    pp.pprint(reviewData[0])\n",
        "\n",
        "  # Save current Movie data to .zip file.\n",
        "  if importZip:\n",
        "    shutil.make_archive('./movie_data', 'zip', './movie_data/')\n",
        "    shutil.make_archive('./review_data', 'zip', './review_data/')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}